## 一、CV

### 2. Backbone

#### 1. Vision Transformer

##### (1)简述ViT模型架构

[ViT论文解读](https://mp.weixin.qq.com/s?__biz=MzI5MDUyMDIxNA==&mid=2247696191&idx=1&sn=06fdf65d2e18b39094790b32d6f6dd86&chksm=ed1b5b85b03feae35eae006692469a0a45ac2c1c4920db3a8d37c93f65e9a1c10558e3ea2f8f&mpshare=1&scene=1&srcid=0929BVl3nYuv8l6hX9OHWOiz&sharer_shareinfo=a8499c4f16f014316f5ddb56421c3eab&sharer_shareinfo_first=a8499c4f16f014316f5ddb56421c3eab#rd)

图像切成patch,线性映射得到PatchEmbedding--> 扩展0维ClassEmbedding--> Position Embedding位置嵌入->TransformerEncoder->MLP Head适应下游业务

![2024年12月08日23-30-47](./assets/2024年12月08日23-30-47.png)

##### (2) 对比CNN，ViT优缺点 

优势

- ==全局建模==能力强：利用自注意力机制，直接捕捉图像长距离依赖关系。
- ==多模适用性==强：各模态架构统一，方便多模态任务中迁移和融合

缺点

- 数据需求量大： ViT缺乏CNN的归纳偏置，需要更多的数据来自动学习到空间不变性等假设。

- 训练难度大： 自注意力机制的计算复杂度较高。

### 4. 对比学习

#### 1. CLIP架构

模型结构： 图像编码器 + 文本编码器，通过对比学习，使得匹配的图像-文本对的特征向量尽可能接近，而不匹配的图像-文本对的特征向量尽可能远离。目的是将图文模态对齐到同一特征空间

#### ❓2. 对比学习常用的损失函数有哪些？

## 二、Transformer

### 1. BERT 和GPT区别？

（1）BERT：Encoder-Only模型，目标是通过双向上下文理解，捕捉词语的深层语义特征，擅长理解任务。

（2）GPT： Decoder-Only 模型，目标是==自回归==持续预测下一个词语 ，擅长生成任务。

### 2.  Encoder和Decoder区别?

1. Encoder：利用自注意力机制==SelfAttention==和前馈神经==FFN==的结构，主要负责将输入编码为高层语义特征。
2. Decoder：额外使用==编码器解码器注意力机制==，主要负责解码高层语义特征，持续预测下一个词，完成序列生成任务。

### 3. 三个Attention作用和区别?   

![2024年12月08日23-21-31](./assets/2024年12月08日23-21-31.png)

### 4.  多头自注意力  

#### （1）多头自注意力作用

1.  每个head可以==关注不同的子空间==，捕捉更丰富的语义信息，增强表达能力。
2.  ==平衡注意力分布==、提高模型的训练效果。

#### （2）mask为啥加很小数?

通过softmax让其输出的注意力权重接近 0，目的是屏蔽不需要计算注意力的元素位置

#### （3） 为什么要除以根号d?   

1. ==归一化==，防止softmax计算值过大，导致梯度消失或训练不稳定。
2. ==平滑注意力分布==，加速模型收敛。

#### （4）多头注意力MHA  和 分组查询注意力GQA区别

![2024年12月08日23-24-30](./assets/2024年12月08日23-24-30.png)

### 5. Transformer如何防止梯度消失？

1. 残差连接：自注意力和前馈层 使用 
2. 层归一化LayerNorm 
3. Dropout
4. 激活函数GELU
5. 缩放点积注意力

### 6. BN和LN的区别？

![2024年12月08日23-25-43](assets/2024年12月08日23-25-43.png)

### 7. 位置编码

#### （1）位置编码的作用

注意力机制并行计算，本身==不具备位置信息==，导致模型无法理解元素之间位置关系。

#### （2）位置编码类型

[links](http://xhslink.com/a/eyexmRriq9G2)

|                  | 作用                                                         | 适用场景 |
| ---------------- | ------------------------------------------------------------ | -------- |
| 绝对位置编码     | ==具体位置==                                                 | 短文本   |
| 相对位置编码     | 不仅具体位置，还捕捉==前后关系==                             | 长度灵活 |
| 旋转位置编码RoPE | 不仅前后关系，通过“角度感知”捕捉==距离多远==，增强长距离理解 | 长文本   |

## 三、大模型

### 1. 大模型有哪些微调方法？

（1）全参微调/冻结部分层：只放开Proj层、Proj+LLM等

（2）参数高效微调PEFT

- P-Tuning：通过添加额外的任务特定参数来引导预模型的行为。
- Adapter-tuning：通过插入小的适配器模块来进行微调，这些适配器模块仅需少量的可训练参数，适用于多任务学习。
- LoRA： 注入可训练的==低秩分解矩阵==   
-  QLoRA：对低秩矩阵量化

### 2. LoRA优势、参数如何初始化、有哪些配置参数？

<img src="./assets/image-20241223上午104125973.png" alt="image-20241223上午104125973" style="zoom:50%;" />

（1）优势：（1）降低训练成本（2）保持原始模型完整性

（2）参数如何初始化？

- 矩阵 A：==正态分布==随机初始化

- 矩阵B ：==全0==初始化

  目的：（1）避免梯度消失（2）控制初始偏移量，避免引入噪声

（3）LoRA参数

- r 低秩矩阵的秩。 `d*d` 分解为`d*r`  `r*d`，取值范围4~**64**之间
- lora_alpha 缩放因子，类似==学习率==用来控制权重更新幅度，平衡学习效率和模型收敛度。
- lora_dropout 丢失概率，防止过拟合。取值范围<0.1

$$
W{\prime} = W + \frac{\alpha}{r} A B
$$



### 3. 如何让大模型推理输出更模糊？

1. 调节温度temperature：控制概率分布的平滑度

   原理：控制softmax函数中的温度参数，改变模型输出的概率分布的平滑度。温度越高，越平滑。

2. Top-k采样：控制选择的候选词数量

   原理：在每次选择下一个词时，模型会根据概率分布选出前k个最可能的词，然后从这k个词中随机选择一个作为输出。增加随机性

3. Top-p采样：根据概率分布的累积阈值进行采样

   原理：设定一个累积概率阈值p，模型将从概率分布中选出最小的一组词，使得这些词的累积概率和大于p。这使得采样过程中，选择的词可能性较高，但同时也允许较低概率的词有机会被选中。

4. 多次采样、随机采样

5. 注入噪声：在输入或隐藏状态中注入噪声

### 4. 如何理解大模型的困惑度PPL？

- 作用：衡量对文本的==预测不确定性==，反映模型的理解能力
- 公式：==类似交叉熵==

### 5. 如何解决训练loss异常？

1. 数据
   - 问题1：可能存在噪声、异常值、标签错误等      解决：数据清洗，去除异常值或修正标签
   - 问题2：场景数据不平衡    解决：丰富数据
2. 训练参数
   - 问题1：学习率过高、batch过小等导致 loss震荡      解决：调整参数
   - 问题2：梯度爆炸/梯度消失      解决：梯度裁剪等
3. 硬件差异
   - 相同参数和配置下，实践中L40s训练无法复现A800结果，低2个点

### 6. 如何分布式训练？

- 并行方案  DP(数据并行)、PP(流水线并行)、TP(张量并行)等

- DeepSpeed ZeRO123   按照==显存瓶颈==优化

  - ZeRO-1：==优化器==状态分片  占3~4倍模型参数
  - ZeRO-2：优化器状态分片+==梯度==分片
  - ZeRO-3：优化器状态分片+梯度分片+==模型参数==分片

  

### 7. VLM模型架构

#### （1）LLaVA

1. 目前有哪些桥接结构？ Qformer、双层MLP、Conv/Linear

2. ⭐️BLIP2和LLava的区别

   |          | BLIP2                                                        | LLaVA                                    |
   | -------- | ------------------------------------------------------------ | ---------------------------------------- |
   | 桥接结构 | Q-former                                                     | MLP                                      |
   | 训练方式 | 训练Q-former<br />                                           | 阶段1训proj  + 阶段2训proj+LLM           |
   | 损失函数 | 图文对比损失ITC :对比图像和文本的相似性。<br />图文匹配损失ITM :判断图像和文本是否匹配。<br />语言建模损失LM  :训练语言模型的生成能力。 | 语言建模损失LM：训练语言模型的生成能力。 |

#### （2）LLaVA-1.5 

1. 数据层面：更多高质量SFT数据

2. 模型模型：（1）ViT分辨率从224到336 （2）Proj用双层MLP(Linear->Gelu->Linear)，代替单线性层

#### （3）LLaVA-1.6(LLaVA-NeXT)

1. 数据层面：构建高质量==SFT数据==，实现更好的推理和OCR能力、更丰富的世界知识
2. 模型层面：动态分辨率==AnyRes==，实现4倍分辨率，保留更多视觉细节，减少幻觉
3. 部署层面：支持==SGLang==高效部署

#### （4）InternVL1.5

<img src="./assets/image-20241225下午20811139.png" alt="image-20241225下午20811139" style="zoom:50%;" />

- 核心方案：AnyRes+PixcelShuffle


#### （5）Mini-Monkey

背景：切分策略引起的==锯齿效应==导致信息丢失，限制了对细节场景的理解能力，尤其对文档理解任务。

方案：

1. 动态分辨率：==多尺度自适应裁剪策略==，即在不同尺度上捕捉图像特征，类似图像金字塔。三层特征包含详细层（提供详细信息） + 适应层（与详细层不重叠的切分，提供多尺度特征） + 全局层（缩略图，提供全局信息）
2. 降低视觉token：尺度压缩机制SCM，通过关注细节层的视觉token仅提取关键视觉特征。

#### （6）InternLM-XComposer

​	整体上PixcelShuffle之后添加特殊tag标记==视觉Token的2D位置关系==

- InternLM-XComposer2-4KHD    仅保留行信息
- InternLM-XComposer-2.5          保留完整2D空间关系

![image-20241223下午114047573](./assets/image-20241223下午114047573.png)

#### （7）Qwen2-VL

两个核心贡献

1. ==原生动态分辨率==：任意分辨率的图像都可以映射成动态数量的视觉tokens。
2. ==多模态旋转位置嵌入（M-RoPE）==：将传统的旋转位置编码分解为==时间、高度和宽度==三个部分，能够同时捕捉一维文本、二维视觉和三维视频的位置信息，增强了多模态数据处理能力。
   - 文本：每个视觉token的时间、高度和宽度使用相同位置ID，退化为传统1维旋转位置编码
   - 图像：每个视觉token的==时间ID不变==，而高度和宽度根据token位置分配不同的ID。看成2帧重复图像的视频
   - 视频：每帧的视觉token==时间ID递增==，而高度和宽度组件遵循与图像相同的ID分配模式。

[Qwen2-VL 模型结构解析](https://zhuanlan.zhihu.com/p/717884243)✅